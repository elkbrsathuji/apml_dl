{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm as l2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "X_d = 4\n",
    "W = np.round(5*(np.random.random((X_d,X_d))-0.5))\n",
    "W[abs(W)>3] = 0\n",
    "y = {\n",
    "    0: lambda x: np.sum(np.dot(x,W),axis=1),\n",
    "    1: lambda x: (x[:,0]*W[0,0]+x[:,1]*W[1,1])*(x[:,2]*W[2,2]+x[:,1]*W[3,3]),\n",
    "    2: lambda x: np.log(np.sum(np.exp(np.dot(x,W)),axis=1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "noise = .01;\n",
    "X = dict(train=5*(np.random.random((1000,X_d))-.5),\n",
    "         test=5*(np.random.random((200,X_d))-.5))\n",
    "Y = {i: {\n",
    "        'train': y[i](X['train'])*(1+np.random.randn(X['train'].shape[0])*.01),\n",
    "        'test': y[i](X['test'])*(1+np.random.randn(X['test'].shape[0])*.01)}\n",
    "     for i in range(len(y))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set hyperparameters, and allocate a structure for learning accuracy and models\n",
    "batch = 100\n",
    "lamb = 0\n",
    "beta1 = .8\n",
    "beta2 = .9\n",
    "eps = 1e-4\n",
    "epochs = 10\n",
    "rate = lambda e: .001\n",
    "models = {\n",
    "    'lin':{i: dict(loss=dict(train=[], test=[])) for i in range(len(y))},\n",
    "    'cnn':{i: dict(loss=dict(train=[], test=[])) for i in range(len(y))}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learn a linear model\n",
    "for fi in range(len(y)):\n",
    "    m_t, v_t = 0, 0\n",
    "    model = models['lin'][fi]\n",
    "    model['w'] = np.zeros(X_d)\n",
    "    for ei in range(epochs):\n",
    "        for bi in range(0, len(Y[fi]['train']), batch):\n",
    "            idx = np.random.randint(0,len(Y[fi]['train']),batch)\n",
    "            xx, yy = X['train'][idx,:], Y[fi]['train'][idx]\n",
    "            p = np.dot(xx, model['w'])\n",
    "            l = (p-yy)**2+lamb*l2(model['w'])\n",
    "            dl_dp = 2*p\n",
    "            dl_dw = 2*model['w']\n",
    "            m_t = beta1*m_t + (1-beta1)*dl_dw\n",
    "            v_t = beta2*v_t + (1-beta2)*(dl_dw)**2\n",
    "            model['w'] -= rate(ei) * (m_t/(1-beta1)) / (np.sqrt(v_t/(1-beta2))+eps)\n",
    "            model['loss']['train'].append(l/batch)\n",
    "            model['loss']['test'].append(np.mean((Y[fi]['test']-np.dot(X['test'],model['w']))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn a toy CNN\n",
    "def forward(model, x):\n",
    "    \"\"\"Fill a dict with forward pass variables\"\"\"\n",
    "    fwd = {}\n",
    "    fwd['x'] = x\n",
    "    fwd['o1'] = np.maximum(0, signal.convolve2d(x, [model['w1']], mode='same'))\n",
    "    fwd['o2'] = np.maximum(0, signal.convolve2d(x, [model['w2']], mode='same'))\n",
    "    fwd['m'] = np.concat([np.maximum(fwd['o1'][:-1],fwd['o1'][1:]),\n",
    "                          np.maximum(fwd['o2'][:-1],fwd['o2'][1:])])\n",
    "    fwd['p'] = np.dot(fwd['m'],model['u']) \n",
    "    return fwd\n",
    "\n",
    "\n",
    "def backprop(model, y, fwd):\n",
    "    \"\"\"Return the derivative of the loss w.r.t. model\"\"\"\n",
    "    \n",
    "    dl_dtheta = np.hstack((dl_dw1, dl_dw2, dl_du))\n",
    "    return dl_dtheta\n",
    "\n",
    "for fi in range(len(y)):\n",
    "    m_t, v_t = 0, 0\n",
    "    model = models['cnn'][fi]\n",
    "    theta = .1 * (np.random.randn(10) - .5)\n",
    "    model['w1'] = model['theta'][:3]\n",
    "    model['w2'] = model['theta'][3:6]\n",
    "    model['u'] = model['theta'][6:]\n",
    "    for ei in range(epochs):\n",
    "        for bi in range(0, len(Y[fi]['train']), batch):\n",
    "            idx = np.random.randint(0, len(Y[fi]['train']), batch)\n",
    "            xx, yy = X['train'][idx, :], Y[fi]['train'][idx]\n",
    "            fwd = forward(model, xx)\n",
    "            l = np.sum((fwd['p'] - yy) ** 2)\n",
    "            dl_dtheta = backprop(model, yy, fwd) + lamb * theta\n",
    "            m_t = beta1 * m_t + (1 - beta1) * dl_dtheta\n",
    "            v_t = beta2 * v_t + (1 - beta2) * (dl_dtheta) ** 2\n",
    "            theta -= rate(ei) * (m_t / (1 - beta1)) / (np.sqrt(v_t / (1 - beta2)) + eps)\n",
    "            model['loss']['train'].append(l / batch)\n",
    "            model['loss']['test'].append(np.mean((Y[fi]['test'] - forward(model, X['test'])['p']) ** 2))\n",
    "\n",
    "\n",
    "# some plots\n",
    "for i in range(len(y)):\n",
    "    plt.subplot(3,2,i*2+1)\n",
    "    l = len(models['cnn'][i]['loss']['train'])\n",
    "    plt.plot(np.arange(l),models['cnn'][i]['loss']['train'],\n",
    "             np.arange(l),models['cnn'][i]['loss']['test'],lw=2)\n",
    "    plt.ylim([0,20])\n",
    "    plt.subplot(3,2,i*2+2)\n",
    "    plt.scatter(forward(models['cnn'][i],X['test'])['p'],Y[i]['test'])\n",
    "    plt.axis('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
